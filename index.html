
<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:17px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-weight:300;
        font-size: 30px;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
	  <title>Semantic Autoencoder for Zero-shot Learning</title>
      <meta property="og:image" content="https://phillipi.github.io/pix2pix/images/teaser_for_fb_v4.png"/>
      <meta property="og:title" content="Image-to-Image Translation with Conditional Adversarial Networks" />
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:36px">Semantic Autoencoder for Zero-shot Learning</span>
	  		  <table align=center width=600px>
	  			  <tr>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:18px"><a href="http://people.eecs.berkeley.edu/~isola/">Elyor Kodirov</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:18px"><a href="https://people.eecs.berkeley.edu/~junyanz/">Tao Xiang</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=100px>
	  					<center>
	  						<span style="font-size:18px"><a href="https://people.eecs.berkeley.edu/~tinghuiz/">Shaogang Gong</a></span>
		  		  		</center>
		  		  	  </td>
	  	              
			  </table>

	  		  <table align=center width=300px>
	  			  <tr>
	  	              <td align=center width=50px>
	  					<center>
	  						<span style="font-size:24px"><a href='https://arxiv.org/pdf/1611.07004v1.pdf'>[Paper]</a>
		  		  		</center>
		  		  	  </td>
	  	              <td align=center width=50px>
	  					<center>
	  						<span style="font-size:24px"><a href='https://github.com/phillipi/pix2pix'>[GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
			  </table>
          </center>

<!--   		  <br><br>
		  <hr> -->

  		  <br>
  		  <table align=center width=850px>
  			  <tr>
  	              <td width=400px>
  					<center>
  	                	<a href="./images/teaser_v3.png"><img class="" src = "./images/teaser_v3.jpg" height="360px"></img></href></a><br>
					</center>
  	              </td>
                </tr>
  	              <td width=400px>
                      <center>
  	                	<span style="font-size:14px"><i>Example results on several image-to-image translation problems. In each case we use the same architecture and objective, simply training on different data.</i>
                      <center>
  	              </td>

  		  </table>

       


  		  <table align=center width=850px>
	  		  <center><h1>Abstract</h1></center>
	  		  <tr>
	  		  	<td>
	  		    </td>
	  		  </tr>
			</table>
				We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.
  		  <br><br>
		  <hr>


	  <!-- NETWORK ARCHITECTURE, TRY THE MODEL -->
 		<center><h1>Try our code</h1></center>

  		  <table align=center width=800px>
			  <tr><center>
				<!-- <span style="font-size:28px">Code coming soon!</span></i>			  	 -->
				<span style="font-size:28px">&nbsp;<a href='https://github.com/phillipi/pix2pix'>[GitHub]</a>

				<!-- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Caffe&nbsp;<a href="http://www.eecs.berkeley.edu/~rich.zhang/projects/2016_colorization/files/demo_v0/colorization_deploy_v0.prototxt">[Prototxt]</a>&nbsp;<a href="https://www.dropbox.com/s/8iq5wm4ton5gwe1/colorization_release_v0.caffemodel">[Model 129MB]--></span></span><i></i>
				<span style="font-size:28px"></a></span>
			  <br>
			  </center></tr>
		  </table>

<!-- <a href="http://www.eecs.berkeley.edu/~rich.zhang/projects/2016_colorization/files/demo_v0/colorization_release_v0.caffemodel">[Model 129MB]</span> -->

      	  <br>
		  <hr>

  		  <table align=center width=300px>
	 		<center><h1>Paper</h1></center>
  			  <tr>
  	              <!--<td width=300px align=left>-->
				  <td><a href="https://arxiv.org/pdf/1611.07004v1.pdf"><img class="layered-paper-big" style="height:150px" src="./images/paper_pdf_thumb.png"/></a></td>
				  <td><span style="font-size:20pt"><a href="https://arxiv.org/pdf/1611.07004v1.pdf"><br>Download [7MB]</a></td>
  	              </td>

                  <!--<td width=200px align=center>
				  <td><a href="./resources/supp.pdf"><img class="layered-paper" style="height:150px" src="./resources/images/supp_pdf_thumb.png"/></a></td>
				  <td><span style="font-size:16pt"><a href="./resources/supp.pdf">Additional details and experiments [1MB]</a></td>
                  </td>-->
              </tr>
  		  </table>
		  <br><br>
        
		  <table align=center width=600px>
			  <tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="./resources/bibtex_arxiv_pix2pix.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table>

		  <br>
		  <hr>

  		  <a name="bw_legacy"></a>
  		  <center><h1>Experiments</h1></center>
          Here we show comprehensive results from each experiment in our paper. Please see the paper for details on these experiments.<br>
          <br>
          
          <table align=center width=600px>
              
        <tr>
        <td valign="top">
  		  <b>Effect of the objective</b><br>
          <a href="./images/index_cityscapes_loss_variations.html">Cityscapes</a><br>
          <a href="./images/index_facades2_loss_variations.html">Facades</a><br>
          <br>
          
  		  <b>Effect of the generator architecture</b><br>
          <a href="./images/index_cityscapes_generator_architecture_variations.html">Cityscapes</a><br>
          <br>
          
  		  <b>Effect of the discriminator patch scale</b><br>
          <a href="./images/index_cityscapes_patchsize_variations.html">Cityscapes</a><br>
          <a href="./images/index_facades2_patchsize_variations.html">Facades</a><br>
          <br>
      </td>
      <td valign="top">
  		  <b>Additional results</b><br>
          <a href="./images/map2sat1_BtoA/latest_net_G_val/index.html">Map to aerial</a><br>
          <a href="./images/sat2map1_AtoB/latest_net_G_val/index.html">Aerial to map</a><br>
          <a href="./images/cityscapes_cGAN_AtoB/latest_net_G_val/index.html">Semantic segmentation</a><br>
          <!--<a href="./images/colorization/index.html">Colorization</a><br>-->
          <a href="./images/att_night/latest_net_G_val/index.html">Day to night</a><br>
          <a href="./images/sketch2photo_handbag/latest_net_G_val/index.html">Edges to handbags</a><br>
          <a href="./images/sketch2photo_shoes/latest_net_G_val/index.html">Edges to shoes</a><br>
          <a href="./images/sketch2photo_handbag/latest_net_G_sketch/index.html">Sketches to handbags</a><br>
          <a href="./images/sketch2photo_shoes/latest_net_G_sketch/index.html">Sketches to shoes</a><br>
          <br>
          
      </td>
      </tr>
          </table>
          
  	  	<hr>
        
	  <table align=center width=1100px>
		  <tr>
              <td width=400px>
     					<left>
      		  <center><h1><a href="https://twitter.com/search?vertical=default&q=pix2pix&src=typd">#pix2pix</a></h1></center>
              People have used our code for many cool applications. Many of them are posted on twitter with the hashtag #pix2pix. Please feel free to check them out <a href="https://twitter.com/search?vertical=default&q=pix2pix&src=typd">here</a>.<br><br>
        </td>
    </tr>
</table>
        
        <hr>
        
        
 		  <a name="related_work"></a>
 		  <table align=center width=1100px>
 			  <tr>
 	              <td width=400px>
 					<left>
  		  <center><h1>Recent Related Work</h1></center>
          
          <a href="https://arxiv.org/abs/1406.2661">Generative adversarial networks</a> have been vigorously explored in the last two years, and many conditional variants have been proposed. Please see the discussion of related work in <a href="https://arxiv.org/pdf/1611.07004v1.pdf">our paper</a>. Below we point out two papers that especially influenced this work: the original GAN paper from Goodfellow et al., and the <a href="https://github.com/soumith/dcgan.torch">DCGAN framework</a>, from which our code is derived.

  		  <br><br>

  		  <!-- <br><br> -->

				Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. <b>Generative Adversarial Networks</b> NIPS, 2014. <a href="https://arxiv.org/pdf/1406.2661v1.pdf">[PDF]</a></br>
                <br>
                Alec Radford, Luke Metz, Soumith Chintala. <b>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</b> ICLR, 2016. <a href="https://arxiv.org/pdf/1511.06434v2.pdf">[PDF]</a></br>

				
			</td>
		 </tr>
	 </table>

	  <br>
	  <hr>
	  <br>

  		  
  		  <table align=center width=1100px>
  			  <tr>
  	              <td>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
					We thank Richard Zhang, Deepak Pathak, and Shubham Tulsiani for helpful discussions. Thanks to Saining Xie for help with the HED edge detector. This work was supported in part by NSF SMA-1514512, NGA NURI, IARPA via Air Force Research Laboratory, Intel Corp, Berkeley Deep Drive, and hardware donations by Nvidia. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, AFRL or the U.S. Government.
			</left>
		</td>
			 </tr>
		</table>

		<br><br>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-24665197-6', 'auto');
  ga('send', 'pageview');

</script>

</body>
</html>
 
